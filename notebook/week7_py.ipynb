{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7\n",
    "\n",
    "Last week we discussed methods for \"big\" data modeling using H2O clusters. This week we'll introduce another big data modeling tool called Spark. Then, we'll discuss naive bayes. We'll finish up with a discussion of AWS Lambda in the context of model deployment. \n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "-   [Modeling with \"Big\" Data](#big-data)\n",
    "-   [Naive Bayes](#naive)\n",
    "    -   [Gaussian](#gaussian)\n",
    "    -   [Multinomial](#multinomial)\n",
    "    -   [Bernoulli](#bernoulli)\n",
    "    -   [Demonstration](#demo)\n",
    "-   [AWS Lambda](#lambda)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='big-data'></a>\n",
    "## Modeling with \"Big\" Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See SparkDemo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='naive'></a>\n",
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes is a method of classification based on Bayes' theorem. Given a set of $n$ features, $\\mathbf{x} = (x_1, \\dots, x_n)$, Naive Bayes predicts a single class $C_k$ from among $K$ possible classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes method can be derived from Bayes theorem, along with assuming the features, $\\mathbf{x}$, are conditionally independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with Bayes Theorem, \n",
    "\n",
    "$p(C_k \\mid \\mathbf{x}) = \\frac{p(C_k) \\ p(\\mathbf{x} \\mid C_k)}{p(\\mathbf{x})}$\n",
    "\n",
    "Since we are only seeking the $\\underset{k \\in \\{1, \\dots, K\\}}{\\operatorname{argmax}}{p(C_k \\mid \\mathbf{x})}$ and $p(\\mathbf{x})$ is the same for every $k$, we don't bother computing $p(\\mathbf{x})$.\n",
    "\n",
    "$\\begin{align}\n",
    "p(C_k \\mid \\mathbf{x}) & \\varpropto p(C_k) \\ p(\\mathbf{x} \\mid C_k) \\\\\n",
    "                       & = \\frac{p(C_k) \\ p(\\mathbf{x} \\cap C_k)}{p(C_k)} \\\\\n",
    "                       & = p(C_k, x_1, \\dots, x_n) \\\\\n",
    "                       & = p(x_1 \\mid x_2, \\dots, x_n, C_k) \\ p(x_2 \\mid x_3, \\dots, x_n, C_k) \\dots   p(x_{n-1} \\mid x_n, C_k) \\ p(x_n \\mid C_k) \\ p(C_k) \\\\\n",
    "                       & = p(x_1 \\mid C_k) \\ p(x_2 \\mid C_k) \\dots   p(x_{n-1} \\mid C_k) \\ p(x_n \\mid C_k) \\ p(C_k) \\\\\n",
    "                       & = p(C_k) \\prod_{i=1}^n p(x_i \\mid C_k)\n",
    "\\end{align}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, $p(C_k)$ can be computed for each $k \\in \\{1, \\dots, K\\}$ from the training dataset by computing the proportion of observations of a given class that make up all observations. You could also assume that each class is equally probable, such that $p(C_k) = \\frac{1}{K}, \\forall k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $p(x_i \\mid C_k)$ for $i \\in {1, \\dots, n}$ can be computed a number of ways, depending on the distribution that we assume for $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gaussian'></a>\n",
    "### Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $x_i$ is continuously-valued and assumed to be normally distributed, then $p(x_i=v \\mid C_k)$ can be computed from the training data with $\\frac{1}{\\sqrt{2\\pi\\sigma^2_k}}\\,e^{ -\\frac{(v-\\mu_k)^2}{2\\sigma^2_k} }$, where $\\mu_k$ is the mean value of $x_i$ in the training data for observations of class $k$, and $\\sigma^2_k$ is the variance of the values of $x_i$ in the training data for observations of class $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multinomial'></a>\n",
    "### Multinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, it might be appropriate to assume that some or all of our features are drawn from a multinomial distribution. \n",
    "\n",
    "$p(\\mathbf{x} \\mid C_k) = \\frac{(\\sum_i x_i)!}{\\prod_i x_i !} \\prod_i {p_{ki}}^{x_i}$\n",
    "\n",
    "This is a typical assumption for document word count features. For example, let's assume that our training dataset consists of the following 4 documents, where each document belongs to one of two classes, A or B ([reference](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html#laplace)). \n",
    "\n",
    "```\n",
    "x1 = # \"Chinese\"\n",
    "x2 = # \"Beijing\"\n",
    "x3 = # \"Shanghai\"\n",
    "x4 = # \"Macao\"\n",
    "x5 = # \"Tokyo\"\n",
    "x6 = # \"Japan\"\n",
    "\n",
    "x1 | x2 | x3 | x4 | x5 | x6 | Class\n",
    "-----------------------------------\n",
    " 2 | 1  | 0  | 0  | 0  | 0  |  A\n",
    "-----------------------------------\n",
    " 2 | 0  | 1  | 0  | 0  | 0  |  A\n",
    "-----------------------------------\n",
    " 1 | 0  | 0  | 1  | 0  | 0  |  A\n",
    "-----------------------------------\n",
    " 1 | 0  | 0  | 0  | 1  | 1  |  B\n",
    "-----------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's say we are asked to predict the class of associated with some document, $\\mathbf{x}$, `Chinese Chinese Chinese Tokyo Japan`, which can be represented as \n",
    "\n",
    "```\n",
    "x1 | x2 | x3 | x4 | x5 | x6 \n",
    "----------------------------\n",
    " 3 | 0  | 0  | 0  | 1  | 1  \n",
    "----------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $(\\sum_i x_i)!$ evaluates to `(3 + 0 + 0 + 0 + 1 + 1)! = 5! = 120`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term $\\prod_i x_i !$ evaluates to `3! * 0! * 0! * 0! * 1! * 1! = 6`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the term $\\prod_i {p_{ki}}^{x_i}$, $p_{ki}$ is the probability of observing the word associated with $x_i$ given the document is of class $k$. We can compute these values at training time from the training data. For example, $p_{A,Chinese}$ equals \n",
    "\n",
    "```\n",
    "# of instances of the word \"Chinese\" in all of the training documents of class A     5\n",
    "-------------------------------------------------------------------------------- =  ---\n",
    "                total # of words in the documents of class A                         8\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we attempt to compute $p_{A,Tokyo}$, we'll get a value of 0. To avoid this situation [*add-one* or *Laplace smoothing*](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html#laplace) can be applied. For example, if we apply Laplace smoothing (0.5), $p_{A,Chinese}$ becomes\n",
    "\n",
    "\n",
    "```\n",
    "  5 + 0.5\n",
    "----------- = 0.5\n",
    " 8 + 0.5*6\n",
    "```\n",
    "\n",
    "and $p_{A,Tokyo}$ becomes\n",
    "\n",
    "```\n",
    "  0 + 0.5\n",
    "----------- = 0.04545455\n",
    " 8 + 0.5*6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bernoulli'></a>\n",
    "### Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your features, $\\mathbf{x}$, it may also be appropriate to assume that they were drawn from a multivariate bernoulli distribution.\n",
    "\n",
    "$p(\\mathbf{x} \\mid C_k) = \\prod_{i=1}^n p_{ki}^{x_i} (1 - p_{ki})^{(1-x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a document classification example, the features would correspond, not to word counts, but to word occurrence/absence. $p_{ki}$ could be computed in the same manner as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='demo'></a>\n",
    "### Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R package `naivebayes` can be used to build naive bayes models. Let's build a model based on the multinomial example above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create our training dataset.\n",
    "\n",
    "```R\n",
    "library(naivebayes)\n",
    "\n",
    "class <- c(\"A\", \"A\", \"A\", \"B\")\n",
    "x1    <- c(2,2,1,1)\n",
    "x2    <- c(1,0,0,0)\n",
    "x3    <- c(0,1,0,0)\n",
    "x4    <- c(0,0,1,0)\n",
    "x5    <- c(0,0,0,1)\n",
    "x6    <- c(0,0,0,1)\n",
    "\n",
    "train <- data.frame(class = class,\n",
    "                    x1 = x1,\n",
    "                    x2 = x2,\n",
    "                    x3 = x3,\n",
    "                    x4 = x4,\n",
    "                    x5 = x5,\n",
    "                    x6 = x6)\n",
    "```\n",
    "\n",
    "```\n",
    "> train\n",
    "  class x1 x2 x3 x4 x5 x6\n",
    "1     A  2  1  0  0  0  0\n",
    "2     A  2  0  1  0  0  0\n",
    "3     A  1  0  0  1  0  0\n",
    "4     B  1  0  0  0  1  1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a `multinomial_naive_bayes` function available as part of the `naive_bayes` package. If we take a look at the documentation, we can see that there is an argument called `laplace`, which has a default value of 0.5. What happens if we set it to 0?\n",
    "\n",
    "```\n",
    "> nb_multinomial <- multinomial_naive_bayes(as.matrix(train[,-1]), train[,1], laplace = 0)\n",
    "Warning message:\n",
    "multinomial_naive_bayes(): There are 5 empty cells leading to zero estimates. Consider Laplace smoothing. \n",
    "\n",
    "> nb_multinomial\n",
    "================================================ Multinomial Naive Bayes ==============\n",
    " \n",
    " Call: multinomial_naive_bayes(x = as.matrix(train[, -1]), y = train[, 1], laplace = 0)\n",
    "\n",
    "-------------------------------------------------------------------------------- \n",
    " \n",
    "Laplace smoothing: 0\n",
    "\n",
    "-------------------------------------------------------------------------------- \n",
    " \n",
    " A priori probabilities: \n",
    "   A    B \n",
    "0.75 0.25 \n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    " \n",
    "        Classes\n",
    "Features     A         B\n",
    "      x1 0.625 0.3333333\n",
    "      x2 0.125 0.0000000\n",
    "      x3 0.125 0.0000000\n",
    "      x4 0.125 0.0000000\n",
    "      x5 0.000 0.3333333\n",
    "      x6 0.000 0.3333333\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "> test <- as.matrix(data.frame(x1=c(3), x2=c(0), x3=c(0), x4=c(0), x5=c(1), x6=c(1)))\n",
    "> predict(nb_multinomial, test, type = \"prob\")\n",
    "       A   B\n",
    "[1,] NaN NaN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and use the default laplace smoothing of 0.5 instead.\n",
    "\n",
    "```\n",
    "> nb_multinomial <- multinomial_naive_bayes(as.matrix(train[,-1]), train[,1])\n",
    "> nb_multinomial\n",
    "\n",
    "================================================ Multinomial Naive Bayes =\n",
    " \n",
    " Call: multinomial_naive_bayes(x = as.matrix(train[, -1]), y = train[, 1])\n",
    "\n",
    "--------------------------------------------------------------------------- \n",
    " \n",
    "Laplace smoothing: 0.5\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    " \n",
    " A priori probabilities: \n",
    "   A    B \n",
    "0.75 0.25 \n",
    "\n",
    "---------------------------------------------------------------------------\n",
    " \n",
    "        Classes\n",
    "Features          A          B\n",
    "      x1 0.50000000 0.25000000\n",
    "      x2 0.13636364 0.08333333\n",
    "      x3 0.13636364 0.08333333\n",
    "      x4 0.13636364 0.08333333\n",
    "      x5 0.04545455 0.25000000\n",
    "      x6 0.04545455 0.25000000\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "> predict(nb_multinomial, test, type = \"prob\")\n",
    "             A         B\n",
    "[1,] 0.4423963 0.5576037\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `naivebayes` package is nice because it allows you to build models when the features, $\\mathbf{x}$, come from various distributions. For example, we can easily construct a naive bayes model with the following dataset.\n",
    "\n",
    "```R\n",
    "n <- 100\n",
    "set.seed(1)\n",
    "class <- sample(c(\"A\", \"B\"), n, TRUE)\n",
    "b1    <- sample(c(TRUE, FALSE), n, TRUE)\n",
    "b2    <- sample(c(TRUE, FALSE), n, TRUE)\n",
    "b3    <- sample(c(TRUE, FALSE), n, TRUE)\n",
    "norm  <- rnorm(n)\n",
    "\n",
    "data <- data.frame(class = class,\n",
    "                   b1 = b1,\n",
    "                   b2 = b2,\n",
    "                   b3 = b3,\n",
    "                   norm = norm)\n",
    "```\n",
    "\n",
    "```\n",
    "> head(data)\n",
    "  class    b1    b2    b3       norm\n",
    "1     A FALSE  TRUE FALSE  0.4094018\n",
    "2     A  TRUE  TRUE  TRUE  1.6888733\n",
    "3     B  TRUE FALSE  TRUE  1.5865884\n",
    "4     B FALSE  TRUE  TRUE -0.3309078\n",
    "5     A FALSE  TRUE  TRUE -2.2852355\n",
    "6     B  TRUE FALSE FALSE  2.4976616\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `naive_bayes` method treats `b1`, `b2`, and `b3` as bernoulli distributed features and `norm` as gaussian\n",
    "\n",
    "```\n",
    "> train <- data[1:90, ]\n",
    "> test <- data[91:100, -1]\n",
    "> nb <- naive_bayes(class ~ ., train)\n",
    "> nb\n",
    "\n",
    "====================================================== Naive Bayes ===================== \n",
    " \n",
    " Call: naive_bayes.formula(formula = class ~ ., data = train)\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    " \n",
    "Laplace smoothing: 0\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    " \n",
    " A priori probabilities: \n",
    "\n",
    "        A         B \n",
    "0.5333333 0.4666667 \n",
    "\n",
    "---------------------------------------------------------------------------------------- \n",
    " \n",
    " Tables: \n",
    "\n",
    "---------------------------------------------------------------------------------------- \n",
    " ::: b1 (Bernoulli) \n",
    "----------------------------------------------------------------------------------------\n",
    "       \n",
    "b1              A         B\n",
    "  FALSE 0.5625000 0.5238095\n",
    "  TRUE  0.4375000 0.4761905\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    " ::: b2 (Bernoulli) \n",
    "----------------------------------------------------------------------------------------\n",
    "       \n",
    "b2              A         B\n",
    "  FALSE 0.3333333 0.4285714\n",
    "  TRUE  0.6666667 0.5714286\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    " ::: b3 (Bernoulli) \n",
    "----------------------------------------------------------------------------------------\n",
    "       \n",
    "b3              A         B\n",
    "  FALSE 0.5000000 0.3809524\n",
    "  TRUE  0.5000000 0.6190476\n",
    "\n",
    "---------------------------------------------------------------------------------------- \n",
    " ::: norm (Gaussian) \n",
    "----------------------------------------------------------------------------------------\n",
    "      \n",
    "norm             A           B\n",
    "  mean 0.006874219 0.013238102\n",
    "  sd   1.003095185 1.121027017\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you deal with features from mixed distributions using scikit-learn's implementation of [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(iris.data, iris.target)\n",
    "gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('gnb.pkl', 'wb') as mf:\n",
    "    pickle.dump(gnb,mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lambda'></a>\n",
    "## AWS Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS [Lambda](https://aws.amazon.com/lambda/) is a service that enables you to run code that would typically reside in a server, without actually setting up and maintaining the server yourself. With Lamda functions, you can delegate typical server administration tasks to AWS. In theory, you don't have to worry about availability and scaling of your service. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the machine learning pipeline we've been discussing throughout the course, Lambda could be used to solve part of the model deployment problem. For example, you could push your model code to the Lambda service. Stakeholders could interact with your model through the Lambda HTTP API, rather than incorporating your model into their own software stack, which may be incompatible with your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through an example of how to set up a simple lambda function and expose it as an HTTP API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can follow the following steps to do this.\n",
    "\n",
    "1. Log into the AWS Console and select the `Lamda` service\n",
    "2. Click on `Create function`\n",
    "3. `Author from scratch`\n",
    "4. In the `Basic information`, we can type a `Function name` of `CS4315Summer2019-Lambda-Instructor`, for example\n",
    "5. Select `Python 3.7` for the `Runtime`\n",
    "6. For `Permissions` -> `Execution role` -> `Use an existing role` -> `Execution role` -> `Lambda_Sagemaker`\n",
    "7. Click `Create function`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. In the `Function code` block, replace the existing code with:\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps({ 'cs4315': 'eeckstrand' })\n",
    "    }\n",
    "```\n",
    "\n",
    "9. Click on `Save`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. In the `Designer` block, click on `+ Add trigger`\n",
    "11. Select `API Gateway`\n",
    "12. For `Security`, select `Open`\n",
    "13. Click `Add`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our API out. We'll use the python `requests` package again. In the `API Gateway` block, copy the `API Endpoint` URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"cs4315\": \"eeckstrand\"}'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "response = requests.post('https://brfnsrikga.execute-api.us-east-2.amazonaws.com/default/CS4315Summer2019-Lambda-Instructor')\n",
    "response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
