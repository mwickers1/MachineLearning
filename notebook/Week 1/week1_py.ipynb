{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Data Preparation and AWS Introduction\n",
    "\n",
    "In this lecture we'll get started with some data preparation exercises and introduce fundamental AWS concepts.\n",
    "\n",
    "**Table of Contents**\n",
    "\n",
    "-   [Data Preparation](#data-preparation)\n",
    "    -   [Pandas DataFrame](#pandas-dataframe)\n",
    "        -   [Creation](#df-creation)\n",
    "            -   [pandas.DataFrame Constructor](#pandasdataframe-constructor)\n",
    "            -   [IO Tools](#df-io-tools)\n",
    "        -   [Exploration and Transformation](#df-exploration-and-transformation)\n",
    "    -   [R data.frame](#r-dataframe)\n",
    "    -   [NumPy ndarray](#numpy-array)\n",
    "        -   [Creation](#ndarray-creation)\n",
    "            -   [numpy.array Constructor](#nparray-constructor)\n",
    "            -   [numpy.genfromtxt](#genfromtxt)\n",
    "        -   [Exploration and Transformation](#np-exploration-and-transformation)\n",
    "-   [Amazon Web Services](#amazon-web-services)\n",
    "    -   [Virtual Private Cloud Components](#virtual-private-cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data-preparation'></a>\n",
    "## Data Preparation\n",
    "\n",
    "Many machine learning tools operate on 2-dimensional, in-memory data structures. As such, there is often a non-trival \n",
    "task of transforming data from its source representation to a representation suitable for machine-learning algorithms.\n",
    "\n",
    "To this end, we are going to cover some of the more popular data structures and transformation techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandas-dataframe'></a>\n",
    "### Pandas DataFrame\n",
    "\n",
    "A Pandas DataFrame is a data structure, that can hold heterogeneous types of data. The official \n",
    "documentation for Pandas DataFrames can be found \n",
    "[here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "We can import the `pandas` package as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df-creation'></a>\n",
    "#### Creation\n",
    "\n",
    "There are numerous ways to construct a Pandas DataFrame. We cover a couple below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pandasdataframe-constructor'></a>\n",
    "##### pandas.DataFrame Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Height': [60, 77, 67], 'Weight': [100, 200, 150], 'Eye Color': ['Brown', 'Green', 'Blue']}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know the data types associated with each of the columns in our DataFrame columns, then look at the \n",
    "`dtypes` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas supports NumPy data types. In short NumPy supports `float`, `int`, `bool`, `timedelta64` and `datetime64` types. \n",
    "More information on NumPy data types can be found \n",
    "[here](https://docs.scipy.org/doc/numpy/reference/arrays.scalars.html). In addition, Pandas has extended the NumPy type \n",
    "system to include additional data types. For example, Pandas supports the `CategoricalDtype` and `DatetimeTZDtype` \n",
    "types. Some of these extended data types are useful for various data transformation tasks. More information on Pandas \n",
    "data types can be found  [here](https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#dtypes).  \n",
    "\n",
    "The DataFrame displayed above has an index for each row. In this case, the first row has an index of 0, and the second\n",
    "row has an index of 1, and so on. By default, when a DataFrame is created the indexes range from 0 to n-1, where n is\n",
    "the number of rows. We can explicitly define the `index` during DataFrame creation as well. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d, index=['A', 'B', 'C'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same result with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=[[60, 100, 'Brown'], [77, 200, 'Green'], [67, 150, 'Blue']], index=['A', 'B', 'C'], columns=['Height', 'Weight', 'Eye Color'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we used the `columns` argument to explicitly define the column names of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df-io-tools'></a>\n",
    "##### IO Tools\n",
    "\n",
    "We don't have to use the `pandas.DataFrame` constructor in order to create DataFrames. Often, our data set will exist\n",
    "in some structured form in persistent storage, such as a file system or database.\n",
    "\n",
    "Pandas has a bunch of IO *connectors*, including \n",
    "[CSV, JSON, SQL, and Google Big Query](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).\n",
    "\n",
    "When your data is persisted as a flatfile CSV, you can use the `pandas.read_csv()` function to make a DataFrame. This \n",
    "function has over 50 possible parameters. This can be expected. `pandas.read_csv()` has the unenviable task of reading\n",
    "data that can come in all shapes an sizes. Refer to the \n",
    "[documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html#pandas.read_csv) to \n",
    "learn about these parameters.\n",
    "\n",
    "Let's say we have a CSV called `iris.csv` in our working directory, which looks like this (first 10 rows):\n",
    "\n",
    "```\n",
    "\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\",\"Species\"\n",
    "5.1,3.5,1.4,0.2,\"setosa\"\n",
    "4.9,3,1.4,0.2,\"setosa\"\n",
    "4.7,3.2,1.3,0.2,\"setosa\"\n",
    "4.6,3.1,1.5,0.2,\"setosa\"\n",
    "5,3.6,1.4,0.2,\"setosa\"\n",
    "5.4,3.9,1.7,0.4,\"virginica\"\n",
    "4.6,3.4,1.4,0.3,\"virginica\"\n",
    "5,3.4,1.5,0.2,\"setosa\"\n",
    "4.4,2.9,1.4,0.2,\"versicolor\"\n",
    "```\n",
    "\n",
    "We can read this CSV into a DataFrame object with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df = pd.read_csv(filepath_or_buffer=\"iris.csv\", header=0, sep=',', dtype={'Sepal.Length': np.float64, 'Sepal.Width': np.float64, 'Petal.Length': np.float64, 'Petal.Width': np.float64, 'Species': 'category'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've told `pandas.read_csv()` that there is a header in the first row by setting `header=0`. If there were no header, \n",
    "we could set this parameter to `None` and specify the column names ourselves with the `names` parameter. Also, notice \n",
    "that we explicity specified the column types with `dtype`. Here's what our DataFrame would look like if we let the\n",
    "the reader figure this stuff out by itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath_or_buffer=\"iris.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it did a pretty good job parsing the file to our specifications, except it assumed the `Species` column is of dtype `object`, which may not be desirable. Not all data is as clean as this example, so you might need to give the parser a little bit of guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='df-exploration-and-transformation'></a>\n",
    "#### Exploration and Transformation\n",
    "\n",
    "Once we've created a DataFrame and before we're ready to create a model, we might need to get more familiar with the \n",
    "data, or transform the data into a form suitable for modeling. \n",
    "\n",
    "DataFrames possess a number of attributes that can give us a better feel for the data.\n",
    "\n",
    "The `shape` attribute gives the dimensions of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris DataFrame has 150 rows and 5 columns.\n",
    "\n",
    "The `columns` attribute gives the column labels of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `describe()` method to compute some basic summary statistics on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the first/last 5 rows of the data use the `head()`/`tail()` mathods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`matplotlib` is a common package for data visualization. We can import it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the histogram of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.hist(alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.plot.box()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`seaborne` is another popular plotting and data visualization package. You can find more information about seaborne [here](https://seaborn.pydata.org/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a pairs plot from our DataFrame as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to take a `subset` of a DataFrame. This is often referred to as `slicing`. We can slice a DataFrame row-wise, column-wise, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DataFrame has a couple of `axes`: a row axis and a column axis. Each axis has an `index` associated with it, which can be used to refer to a particular row or column, for example. Observe the `label` indexes of the Iris DataFrame with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first list element is row `label` index associated with the row axis, and the second list element is the column `label` index associated with the column axis. The row `label` index is simply a list of integers from 0 to 149. You'll notice that the column `label` index is a list of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.axes[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.axes[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.loc[]` is the slicing operator for `label` indexes. `.iloc[]` is another common slicing operator, but it operates on `position` indexes, which we will introduce soon. \n",
    "\n",
    "In general, slice operations take the following form `df.loc[row_indexer,column_indexer]`. If the `column_indexer` is not specified, then it is assumed to be the `null` slice, or `:`, which means \"all columns.\" So, we could ask for the first row with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask for the element in the first row and first column with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0,'Sepal.Length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to think that you can retrieve the same value with `df.loc[0,0]`, but this will surely return an error. Remember that `.loc[]` operates on `label` indexes, not `position` indexes. Refer back to `df.axes` to understand the acceptable label index values for `.loc[]`. For the Iris DataFrame, the acceptable values are `'Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width', and 'Species'`. `0` is not an acceptable label index value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.iloc[]` operator, on the other hand operates on `position` indexes. The position indexes are implicitly defined for DataFrames and they range from `0` to `# of rows - 1` for the row index, and from `0` to `# of columns - 1` for the column index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the element in the first row and first column with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`df.iloc[0,'Sepal.Length']` will fail because `'Sepal.Length'` is not an acceptable `position` index value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common use case of DataFrame slicing is to split a dataset into training and test sets, which consist of observations randomly drawn from the original dataset, without replacement. \n",
    "\n",
    "One way to do this would be to decide approximately what percent of the original data the train and test sets will assume. Let's say that we want our training set to have about 75% of the original data and the test set 25%. We can first generate a list of 150 numbers between 0 and 1 (uniformly distributed) with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_arr = np.random.rand(len(df))\n",
    "rand_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_arr.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that approximately 75% of the observations will be > 0.25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_arr_bool = rand_arr > 0.25\n",
    "rand_arr_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_arr_bool.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rand_arr_bool) / len(rand_arr_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can slice the original DataFrame based on this boolean NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.iloc[rand_arr_bool,:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df.loc[~rand_arr_bool,:]\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.loc[]` or `.iloc[]` slicing operators work equally well here. Refer to the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#different-choices-for-indexing) to understand all of the allowable inputs to these operators. We used a boolean array in the above sampling example, but other inputs are possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes `normalizing` your dataset may lead to superior models and/or faster training times. One common normalization technique is `standardization`. For each element, you subtract the mean and divide by the standard deviation of the column to which the element belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the mean of a column with the `mean()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mean()` can also operate on the entire DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_means = df.mean()\n",
    "col_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(col_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_means.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `Species` column, which is categorical, is left off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the standard deviation in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_s = df.std()\n",
    "col_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(col_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subtract the column mean from each element in the Dataframe simply by using the `-` operator. Behind the scenes, Pandas has defined what it means to subtract a `pandas.core.series.Series` object from a `pandas.core.frame.DataFrame` object. Learn more about the semantics of these operations [here](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html#binary-operator-functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df.iloc[:,0:4] - col_means\n",
    "df_mean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, an element-wise subtraction of the column means across for each row was performed. Now, let's perform an element-wise division of the column standard deviations for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = df_mean / df.std()\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_norm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the labels attached to this normalized data. We can either update the original dataset with the new data, or simply add the label column to the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[:,0:4] = df_norm\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = df_norm.join(df.iloc[:,4])\n",
    "df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we had to assign the result of `df_norm.join(df.iloc[:,4])` to `df_norm`. The `join()` operation return a copy of the dataset. It doesn't alter-in-place `df_norm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's desireable to take the log of each value in a column. There are a [ton](https://pandas.pydata.org/pandas-docs/stable/reference/frame.html) of Pandas operators for DataFrames, but we have to rely on the NumPy `log` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(filepath_or_buffer=\"iris.csv\", header=0, sep=',', dtype={'Sepal.Length': np.float64, 'Sepal.Width': np.float64, 'Petal.Length': np.float64, 'Petal.Width': np.float64, 'Species': 'category'})\n",
    "df.iloc[:,0] = np.log(df.iloc[:,0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='r-dataframe'></a>\n",
    "### R data.frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See `week1_R.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NumPy ndarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy `ndarray` is another popular data structure. It is used to hold multidimensional, homogeneous data. `ndarray`s are homogeneous because they can only hold objects of the same [dtype](https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ndarray-creation'></a>\n",
    "#### Creation\n",
    "\n",
    "There are numerous ways to construct a NumPy `ndarray`. We cover a couple below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nparray-constructor'></a>\n",
    "##### numpy.array Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1,2,3],[4,5,6]])\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions of a numpy are referred to as `axes`. The above array has 2 axes, for example. We can find this information by accessing the `ndim` attribute of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know the size of each of the axes, or dimensions, then we can access the `shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we see that the first dimension has 2 elements, and the second dimension has 3 elements. We can think of this array as a table with 2 rows and 3 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know the `dtype` of an array, then we can access that attribute as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='genfromtxt'></a>\n",
    "##### numpy.genfromtxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all likelihood our data will probably reside on disk in CSV format. We can use the `genfromtxt()` function to read this data into a ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "iris_arr = genfromtxt('iris.csv', delimiter=',')\n",
    "iris_arr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened to the column headers and the `Species` classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left off the `dtype` argument to the genfromtxt function, so it had to make a guess. Specifying the dtype as `object`, gives use the following array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_arr2 = genfromtxt('iris.csv', delimiter=',', dtype='object')\n",
    "iris_arr2[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert pandas `DataFrame`s into numpy `ndarray`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(filepath_or_buffer=\"iris.csv\", header=0, sep=',', dtype={'Sepal.Length': np.float64, 'Sepal.Width': np.float64, 'Petal.Length': np.float64, 'Petal.Width': np.float64, 'Species': 'category'})\n",
    "iris_arr3 = df.values\n",
    "iris_arr3[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='np-exploration-and-transformation'></a>\n",
    "#### Exploration and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the first row of the original array. We can do this with the `delete()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(np.delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_arr = np.delete(iris_arr, (0), axis=0)\n",
    "iris_arr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's delete the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_arr = np.delete(iris_arr, (4), axis=1)\n",
    "iris_arr[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already seen how to slice an ndarray along the first (row) dimension with `iris_arr[:5]`. Let's slice along the second (column) dimension now too. Let's extract the first 5 rows of the first 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = iris_arr[:5,:2]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*np.sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = iris_arr[5:10,:2]\n",
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more operations that we can perform on NumPy arrays and we'll see a lot of them as we proceed through the course. [Here's](https://docs.scipy.org/doc/numpy/reference/) a link to the main NumPy reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='amazon-web-services'></a>\n",
    "## Amazon Web Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of cloud services and providers seem to be constantly growing. [Microsoft Azure](https://azure.microsoft.com/en-us/), [Google Cloud](https://cloud.google.com/), and [Amazon Web Services](https://aws.amazon.com/) are just a few. Basic services include storage, databases, computing resources, and networking infrastructure, to name a few. However, providers appear to be moving \"up the stack,\" providing domain  specific software services, such as [IDEs](https://aws.amazon.com/cloud9/) and machine learning modeling tools. AWS [Sagemaker](https://aws.amazon.com/sagemaker/), [Azure Machine Learning](https://azure.microsoft.com/en-us/services/machine-learning-service/), and Google [AI Products](https://cloud.google.com/products/ai/) are a few cloud-based machine learning platforms.\n",
    "\n",
    "While these services are not necessarily required to build machine learning pipelines and products, they might provide some advantages over developing custom solutions. In this course, we will cover some basic cloud concepts using AWS, which should help you get more comfortable building and interacting with cloud services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='virtual-private-cloud'></a>\n",
    "### Virtual Private Cloud (VPC) Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [*VPC*](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html) is a logically isolated section of the AWS Cloud where you can launch AWS resources in a network that you define. You have complete control over the VPC, including the selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='default-vpc-diagram.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VPC has a range of IP addresses associated with it, which you specify upon creation. The IP address range is specified using [CIDR](https://tools.ietf.org/html/rfc4632) block notation. For IPv4 addresses, the *network* portion of a 32-bit CIDR block is specified with the 8-bit value after the `/` symbol. For example, in the CIDR block `21.39.16.0/20` the first 20 bits (`000010101.00100111.0001`) specify the network address, and the last 12 bits are zeros (`0000.00000000`). This leaves 4096 (`2^12`) values for end systems within this VPC.\n",
    "\n",
    "Often, the VPC IP address space is further divided into *Subnets*. A subnet is simply a sub-range of IP addresses of the parent range. For example, CIDR block `21.39.24.0/21` (`000010101.00100111.00011000.00000000`) is a subnet of `21.39.16.0/20`, with 2048 (`2^11`) values for end systems within this subnet. \n",
    "\n",
    "When you create a VPC, you must specify an IPv4 CIDR block for the VPC. Blocks are can have a netmask between `/16` and `/28`. Amazon recommends using CIDR blocks from the private IPv4 address ranges as specified in RFC 1918:\n",
    "```\n",
    "10.0.0.0 - 10.255.255.255 (10/8 prefix)\n",
    "172.16.0.0 - 172.31.255.255 (172.16/12 prefix)\n",
    "192.168.0.0 - 192.168.255.255 (192.168/16 prefix)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A VPC exists within an AWS *Region*, which corresponds to a geographical area. Not all regions support all AWS services! Check this [chart](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/). Regions are further split into [*Availability Zones*](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html). A VPC spans all availability zones within a region. Availability zones are isolated from each other. To enhance reliability, you can place backup services in subnets in different availability zones. A subnet must exist in a single availability zone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each subnet has a [*Route Table*](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Route_Tables.html), which specifies the route for traffic leaving the subnet. In the above picture, for example, any traffic with a destination within the VPC (`172.31.0.0/16`) is routed locally, but traffic destined for *any* (`0.0.0.0/0`) other destination is routed to the *Internet Gateway*. The most specific route that matches the traffic is used to determine how to route the traffic. When you create a VPC, it automatically has a main route table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An [*Internet Gateway*](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html) is a VPC component that allows communication between instances in your VPC and the internet. To enable access to or from the internet for instances in a VPC subnet, you must do the following:\n",
    "\n",
    "- Attach an internet gateway to your VPC.\n",
    "- Ensure that your subnet's route table points to the internet gateway.\n",
    "- Ensure that instances in your subnet have a globally unique IP address (public IPv4 address, Elastic IP address, or IPv6 address).\n",
    "- Ensure that your network access control and security group rules allow the relevant traffic to flow to and from your instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [*Security Group*](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html) acts as a virtual firewall for your instance to control inbound and outbound traffic. Security groups act at the instance level, not the subnet level. Therefore, each instance in a subnet in your VPC could be assigned to a different set of security groups. If you don't specify a particular group at launch time, the instance is automatically assigned to the default security group for the VPC.\n",
    "\n",
    "For each security group, you add rules that control the inbound traffic to instances, and a separate set of rules that control the outbound traffic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='default-security-group.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more security group example rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Image(filename='security-group-examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first lab assignment I'll ask you to create a simple VPC with the above components, along with an EC2 instance, using [CloudFormation](https://aws.amazon.com/cloudformation/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
